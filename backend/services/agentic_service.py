# services/agentic_service.py 
from typing import List, Dict, Any, Optional
import json
import requests
from datetime import datetime
import asyncio
from google.cloud import firestore
from core.database import db
from services.llm_service import llm_service
from services.web_search_service import web_search_service 
import httpx
import time

class AgenticService:
    def __init__(self):
        self.available_actions = {
            "web_search": self.web_search,
            "data_analysis": self.analyze_data,
            "document_processing": self.process_documents,
            "code_generation": self.generate_code,
            "knowledge_update": self.update_knowledge,
            "summary_generation": self.generate_summary
        }
        
        self.action_descriptions = {
            "web_search": "Recherche d'informations sur le web",
            "data_analysis": "Analyse de donn√©es et g√©n√©ration d'insights",
            "document_processing": "Traitement et extraction d'informations de documents",
            "code_generation": "G√©n√©ration de code dans diff√©rents langages",
            "knowledge_update": "Mise √† jour de la base de connaissances",
            "summary_generation": "G√©n√©ration de r√©sum√©s et synth√®ses"
        }
    
    async def execute_action(self, action_name: str, parameters: Dict, user_id: str) -> Dict:
        """Ex√©cute une action agentique"""
        if action_name not in self.available_actions:
            return {"error": f"Action non support√©e: {action_name}"}
        
        try:
            result = await self.available_actions[action_name](parameters, user_id)

             # Enrichir la base de connaissances avec les r√©sultats de l'action
            if action_name in ["web_search", "data_analysis", "document_processing"]:
                await self.enrich_knowledge_from_action(action_name, parameters, user_id, result)

            return {
                "action": action_name,
                "parameters": parameters,
                "result": result,
                "timestamp": datetime.now().isoformat(),
                "status": "success"
            }
        except Exception as e:
            return {
                "action": action_name,
                "parameters": parameters,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
                "status": "error"
            }


    async def web_search(self, params: Dict, user_id: str) -> Dict:
        """Recherche web avec le nouveau service optimis√©"""
        query = params.get("query", "")
        max_results = params.get("max_results", 8)
        
        # Nettoyer et optimiser la requ√™te
        query = self._optimize_search_query(query)
        
        if not query.strip():
            return {
                "error": "Requ√™te de recherche vide",
                "query": query,
                "results": [],
                "result_count": 0
            }
        
        print(f"üîç Recherche web optimis√©e: '{query}'")
        
        try:
            # Utiliser le service de recherche am√©lior√©
            search_result = await web_search_service.search_with_fallback(query, max_results)
            
            if search_result.get("has_web_results", False):
                results = search_result["results"]
                print(f"‚úÖ Recherche r√©ussie: {len(results)} r√©sultats")
                
                # Formater pour l'affichage
                return {
                    "query": query,
                    "results": results,
                    "result_count": len(results),
                    "sources_used": search_result.get("sources_used", []),
                    "source": "web_search_success",
                    "status": "success"
                }
            else:
                # Fallback vers LLM avec contexte am√©lior√©
                print("‚ö†Ô∏è Pas de r√©sultats web, utilisation du fallback LLM")
                return await self._enhanced_llm_fallback(query)
                
        except Exception as e:
            print(f"‚ùå Erreur recherche web: {e}")
            return await self._enhanced_llm_fallback(query)

    def _optimize_search_query(self, query: str) -> str:
        """Optimise la requ√™te de recherche pour de meilleurs r√©sultats"""
        # Supprimer les mots superflus
        stop_words = [
            "rechercher", "chercher", "trouver", "donner", "montrer", 
            "sur", "des", "du", "de", "la", "le", "les", "un", "une",
            "pour", "afin", "dans", "avec", "sans"
        ]
        
        # Nettoyer la requ√™te
        words = query.lower().split()
        filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Reformuler la requ√™te
        optimized_query = " ".join(filtered_words)
        
        # Cas sp√©cifiques pour les recherches temporelles
        if "2025" in query:
            # Pour les recherches temporelles, garder l'ann√©e
            optimized_query = optimized_query.replace("2025", "") + " 2025"
        
        return optimized_query.strip()


    # async def web_search(self, params: Dict, user_id: str) -> Dict:
    #     """Recherche web avec le nouveau service optimis√©"""
    #     query = params.get("query", "")
    #     max_results = params.get("max_results", 5)
        
    #     if not query.strip():
    #         return {
    #             "error": "Requ√™te de recherche vide",
    #             "query": query,
    #             "results": [],
    #             "result_count": 0
    #         }
        
    #     print(f"üîç Recherche web demand√©e: '{query}'")
        
    #     try:
    #         # Utiliser le service de recherche am√©lior√©
    #         search_result = await web_search_service.search_with_fallback(query, max_results)
            
    #         if search_result.get("has_web_results", False):
    #             results = search_result["results"]
    #             print(f"‚úÖ Recherche r√©ussie: {len(results)} r√©sultats")
                
    #             # Formater pour l'affichage
    #             return {
    #                 "query": query,
    #                 "results": results,
    #                 "result_count": len(results),
    #                 "sources_used": search_result.get("sources_used", []),
    #                 "source": "web_search_success",
    #                 "status": "success"
    #             }
    #         else:
    #             # Fallback vers LLM avec contexte am√©lior√©
    #             print("‚ö†Ô∏è Pas de r√©sultats web, utilisation du fallback LLM")
    #             return await self._enhanced_llm_fallback(query)
                
    #     except Exception as e:
    #         print(f"‚ùå Erreur recherche web: {e}")
    #         return await self._enhanced_llm_fallback(query)

    async def _enhanced_llm_fallback(self, query: str) -> Dict:
        """Fallback LLM am√©lior√© avec prompt contextuel"""
        try:
            # Analyser la requ√™te pour adapter le prompt
            is_recent_query = any(year in query for year in ["2024", "2025", "2026"])
            is_tech_query = any(word in query.lower() for word in ["llm", "ai", "ia", "intelligence artificielle", "mod√®le", "algorithme"])
            
            # Prompt adaptatif selon le type de requ√™te
            if is_recent_query and is_tech_query:
                enhanced_prompt = f"""
    Vous √™tes un expert en IA et technologies r√©centes. L'utilisateur demande : "{query}"

    Cette question concerne des informations potentiellement tr√®s r√©centes. Voici ce que je peux vous dire :

    CONTEXTE TEMPOREL : Mes donn√©es s'arr√™tent en janvier 2025, mais je peux vous aider avec :

    1. **Tendances observ√©es jusqu'en janvier 2025**
    2. **Projections bas√©es sur les d√©veloppements r√©cents**
    3. **Orientations de recherche prometteuses**

    Pour les LLM open source sp√©cifiquement, voici les d√©veloppements notables jusqu'√† ma date de coupure :

    ‚Ä¢ **Llama 3.x** : Continuation de la s√©rie Meta avec des am√©liorations significatives
    ‚Ä¢ **Mistral** : Mod√®les fran√ßais performants avec versions 7B et plus
    ‚Ä¢ **Gemma** : Famille de mod√®les Google ouverts
    ‚Ä¢ **Qwen** : Mod√®les multilingues d'Alibaba
    ‚Ä¢ **DeepSeek** : Mod√®les sp√©cialis√©s en raisonnement et code

    **TENDANCES 2025 OBSERV√âES** :
    - Optimisation des mod√®les pour l'efficacit√© (moins de param√®tres, m√™me performance)
    - Sp√©cialisation par domaine (code, science, multimodal)
    - Am√©lioration des capacit√©s multilingues
    - Focus sur la transparence et l'explicabilit√©

    **POUR LES INFORMATIONS LES PLUS R√âCENTES**, je recommande de consulter :
    - Hugging Face Model Hub
    - Papers With Code
    - ArXiv pour les publications r√©centes
    - GitHub des projets open source

    R√©pondez-moi si vous voulez des d√©tails sur un aspect particulier.
    """
            else:
                enhanced_prompt = f"""
    Vous √™tes un assistant IA sp√©cialis√©. L'utilisateur pose cette question : "{query}"

    Bien que mes donn√©es s'arr√™tent en janvier 2025, je peux vous fournir :

    1. **Informations factuelles** disponibles jusqu'√† cette date
    2. **Analyse des tendances** observ√©es
    3. **Recommandations** pour obtenir les informations les plus r√©centes
    4. **Contexte g√©n√©ral** sur le sujet

    Voici ma r√©ponse d√©taill√©e sur votre question :
    """

            # Appel au LLM avec le prompt am√©lior√©
            llm_response = await llm_service.get_response([
                {"role": "user", "content": enhanced_prompt}
            ])
            
            return {
                "query": query,
                "results": [{
                    "title": f"R√©ponse experte sur : {query}",
                    "content": llm_response,
                    "source": "llm_enhanced_fallback",
                    "note": "R√©ponse bas√©e sur les connaissances IA (donn√©es jusqu'en janvier 2025) avec suggestions pour informations r√©centes",
                    "type": "enhanced_response"
                }],
                "result_count": 1,
                "source": "llm_enhanced_fallback",
                "status": "fallback_enhanced",
                "recommendation": "Pour les informations les plus r√©centes, consultez les sources sp√©cialis√©es mentionn√©es dans la r√©ponse."
            }
            
        except Exception as e:
            print(f"‚ùå Erreur fallback LLM: {e}")
            return {
                "query": query,
                "results": [{
                    "title": "Service temporairement indisponible",
                    "content": "Les services de recherche et l'assistant IA rencontrent des difficult√©s techniques. Veuillez r√©essayer dans quelques instants.",
                    "source": "error_fallback",
                }],
                "result_count": 1,
                "source": "error",
                "status": "error"
            }

    async def enrich_knowledge_from_action(self, action_name: str, params: Dict, user_id: str, results: Dict):
        """Enrichit la base de connaissances √† partir des r√©sultats d'une action agentique"""
        try:
            if action_name == "web_search" and results.get("results"):
                # Convertir les r√©sultats en format document
                documents = []
                for result in results["results"]:
                    content = f"Titre: {result.get('title', '')}\n\nContenu: {result.get('content', '')}"
                    
                    document = {
                        "page_content": content,
                        "metadata": {
                            "source": result.get("source", "web_search"),
                            "url": result.get("url", ""),
                            "query": results.get("query", ""),
                            "added_via": "agentic_action",
                            "user_id": user_id,
                            "action": action_name,
                        }
                    }
                    documents.append(document)
                
                # Utiliser le service d'enrichissement pour ajouter √† la base
                if documents:
                    from langchain.schema import Document
                    langchain_docs = [
                        Document(page_content=doc["page_content"], metadata=doc["metadata"])
                        for doc in documents
                    ]
                    
                    # Ajouter √† la base vectorielle
                    from core.vectorstore import vector_store
                    added_count = vector_store.add_documents(
                        langchain_docs, 
                        user_id=user_id
                    )
                    print(f"‚úÖ {added_count} documents ajout√©s √† la base via action agentique")

                    return added_count
                    
        except Exception as e:
            print(f"‚ùå Erreur enrichissement depuis action agentique: {e}")
        
        return 0


    # Reste des m√©thodes inchang√©es...
    async def analyze_data(self, params: Dict, user_id: str) -> Dict:
        """Analyse de donn√©es am√©lior√©e"""
        data = params.get("data", {})
        analysis_type = params.get("analysis_type", "summary")
        data_provided = params.get("data_provided", True)
        
        try:
            if not data_provided or not data:
                return {
                    "error": "Aucune donn√©e fournie pour l'analyse",
                    "data_provided": False,
                    "suggestion": "Veuillez fournir des donn√©es √† analyser. Exemple: 'Analyse ces chiffres: 10, 20, 30, 40'"
                }
            
            if analysis_type == "numeric" and isinstance(data, list):
                # Analyse num√©rique basique
                numbers = [float(x) for x in data if str(x).isdigit()]
                if numbers:
                    return {
                        "count": len(numbers),
                        "sum": sum(numbers),
                        "average": sum(numbers) / len(numbers),
                        "min": min(numbers),
                        "max": max(numbers),
                        "type": "numeric_analysis",
                        "data_provided": True
                    }
            
            elif analysis_type == "text":
                # Analyse textuelle basique
                if isinstance(data, str):
                    words = data.split()
                    return {
                        "word_count": len(words),
                        "character_count": len(data),
                        "type": "text_analysis",
                        "data_provided": True
                    }
            
            return {
                "error": "Type d'analyse non support√© ou donn√©es invalides",
                "data_received": str(data)[:100],
                "analysis_type": analysis_type,
                "data_provided": True
            }
                
        except Exception as e:
            return {"error": f"Erreur analyse donn√©es: {str(e)}", "data_provided": data_provided}

    
    async def process_documents(self, params: Dict, user_id: str) -> Dict:
        """Traitement de documents avanc√©"""
        from services.rag_service import rag_service
        
        action = params.get("action", "summarize")
        content = params.get("content", "")
        
        try:
            if action == "summarize":
                # Utiliser le LLM pour r√©sumer
                prompt = f"R√©sumez ce document de mani√®re concise et informative:\n\n{content}"
                summary = await llm_service.get_response([{"role": "user", "content": prompt}])
                
                return {
                    "action": "summarize",
                    "original_length": len(content),
                    "summary": summary,
                    "summary_length": len(summary)
                }
            
            elif action == "extract_keypoints":
                prompt = f"Extrayez les points cl√©s de ce document sous forme de liste:\n\n{content}"
                keypoints = await llm_service.get_response([{"role": "user", "content": prompt}])
                
                return {
                    "action": "extract_keypoints",
                    "keypoints": keypoints
                }
            
            return {"error": "Action de document non support√©e"}
            
        except Exception as e:
            return {"error": f"Erreur traitement document: {str(e)}"}
    
    async def generate_code(self, params: Dict, user_id: str) -> Dict:
        """G√©n√©ration de code"""
        language = params.get("language", "python")
        task = params.get("task", "")
        requirements = params.get("requirements", "")
        
        try:
            prompt = f"""
            G√©n√®re du code {language} pour: {task}
            Exigences: {requirements}
            
            Fournis le code complet avec des commentaires explicites.
            """
            
            code = await llm_service.get_response([{"role": "user", "content": prompt}])
            
            return {
                "language": language,
                "task": task,
                "code": code,
                "code_length": len(code)
            }
            
        except Exception as e:
            return {"error": f"Erreur g√©n√©ration code: {str(e)}"}
    
    async def update_knowledge(self, params: Dict, user_id: str) -> Dict:
        """Mise √† jour de la base de connaissances"""
        from services.rag_service import rag_service
        
        text = params.get("text", "")
        source = params.get("source", "agentic_update")
        
        try:
            if text:
                added_count = rag_service.add_knowledge_documents(
                    texts=[text],
                    metadata=[{"source": source, "type": "agentic_update"}],
                    user_id=user_id
                )
                
                return {
                    "action": "knowledge_update",
                    "chunks_added": added_count,
                    "source": source
                }
            
            return {"error": "Aucun texte fourni pour la mise √† jour"}
            
        except Exception as e:
            return {"error": f"Erreur mise √† jour connaissances: {str(e)}"}
    
    async def generate_summary(self, params: Dict, user_id: str) -> Dict:
        """G√©n√©ration de r√©sum√© de conversation"""
        conversation_id = params.get("conversation_id", "")
        
        try:
            if conversation_id:
                # R√©cup√©rer la conversation
                conv_ref = db.collection('users').document(user_id).collection('conversations').document(conversation_id)
                conversation = conv_ref.get()
                
                if conversation.exists:
                    conv_data = conversation.to_dict()
                    messages = conv_data.get('messages', [])
                    
                    # Pr√©parer le contenu √† r√©sumer
                    content = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
                    
                    prompt = f"""
                    R√©sumez cette conversation en mettant en √©vidence:
                    1. Les questions principales pos√©es
                    2. Les r√©ponses importantes
                    3. Les conclusions ou actions √† prendre
                    
                    Conversation:
                    {content}
                    """
                    
                    summary = await llm_service.get_response([{"role": "user", "content": prompt}])
                    
                    return {
                        "conversation_id": conversation_id,
                        "message_count": len(messages),
                        "summary": summary
                    }
            
            return {"error": "Conversation non trouv√©e"}
            
        except Exception as e:
            return {"error": f"Erreur g√©n√©ration r√©sum√©: {str(e)}"}
    
    def get_available_actions(self) -> List[Dict]:
        """Retourne la liste des actions disponibles"""
        return [
            {
                "name": name,
                "description": desc,
                "parameters": self._get_action_parameters(name)
            }
            for name, desc in self.action_descriptions.items()
        ]
    
    def _get_action_parameters(self, action_name: str) -> List[Dict]:
        """Retourne les param√®tres requis pour chaque action"""
        parameters = {
            "web_search": [
                {"name": "query", "type": "string", "required": True, "description": "Termes de recherche"},
                {"name": "max_results", "type": "integer", "required": False, "description": "Nombre max de r√©sultats"}
            ],
            "data_analysis": [
                {"name": "data", "type": "object", "required": True, "description": "Donn√©es √† analyser"},
                {"name": "analysis_type", "type": "string", "required": False, "description": "Type d'analyse"}
            ],
            "document_processing": [
                {"name": "action", "type": "string", "required": True, "description": "Action (summarize, extract_keypoints)"},
                {"name": "content", "type": "string", "required": True, "description": "Contenu du document"}
            ],
            "code_generation": [
                {"name": "language", "type": "string", "required": True, "description": "Langage de programmation"},
                {"name": "task", "type": "string", "required": True, "description": "Description de la t√¢che"},
                {"name": "requirements", "type": "string", "required": False, "description": "Exigences suppl√©mentaires"}
            ],
            "knowledge_update": [
                {"name": "text", "type": "string", "required": True, "description": "Texte √† ajouter"},
                {"name": "source", "type": "string", "required": False, "description": "Source de l'information"}
            ],
            "summary_generation": [
                {"name": "conversation_id", "type": "string", "required": True, "description": "ID de la conversation"}
            ]
        }
        
        return parameters.get(action_name, [])

# Instance globale
agentic_service = AgenticService()
